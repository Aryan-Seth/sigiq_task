{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d31a76",
   "metadata": {},
   "source": [
    "# TTS WebSocket Evaluation\n",
    "\n",
    "Runs end-to-end measurements for:\n",
    "- Latency (client-side optimization metric: first real text sent â†’ first audio received)\n",
    "- Audio quality sanity checks (RMS, peak, clipping, silence)\n",
    "- Optional alignment accuracy vs WhisperX (if installed)\n",
    "\n",
    "Prereqs:\n",
    "- Server running at `ws://localhost:8000/tts`\n",
    "- `websockets`, `numpy` installed (already in requirements.txt)\n",
    "- For alignment eval: `whisperx`, `torch`, `torchaudio` in the same environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54be40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, base64, json, time, math, sys\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import websockets\n",
    "\n",
    "# Optional: WhisperX aligner\n",
    "try:\n",
    "    from app.aligner import WhisperXAligner\n",
    "    HAVE_WHISPERX = True\n",
    "except Exception:\n",
    "    HAVE_WHISPERX = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcm_from_b64(audio_b64: str) -> np.ndarray:\n",
    "    pcm = base64.b64decode(audio_b64)\n",
    "    return np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "async def run_session(text: str, uri: str = \"ws://localhost:8000/tts\", chunk_size: int = 512, delay: float = 0.0):\n",
    "    # Streams text, returns audio array, sample rate, alignments, and timing metrics.\n",
    "    # Alignment timestamps are absolute (ms from start of session).\n",
    "    pcm_chunks: List[np.ndarray] = []\n",
    "    align_chars: List[str] = []\n",
    "    align_starts: List[float] = []\n",
    "    align_durs: List[float] = []\n",
    "\n",
    "    sr = 44100\n",
    "    samples_so_far = 0\n",
    "    t_first_send = None\n",
    "    t_first_audio = None\n",
    "    t_start = None\n",
    "    t_last_audio = None\n",
    "\n",
    "    async with websockets.connect(uri, max_size=None) as ws:\n",
    "        await ws.send(json.dumps({\"text\": \" \", \"flush\": False}))\n",
    "\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            if t_start is None:\n",
    "                t_start = time.perf_counter()\n",
    "            await ws.send(json.dumps({\"text\": text[i:i+chunk_size], \"flush\": False}))\n",
    "            if t_first_send is None:\n",
    "                t_first_send = time.perf_counter()\n",
    "            await asyncio.sleep(delay)\n",
    "\n",
    "        await ws.send(json.dumps({\"text\": \"\", \"flush\": True}))\n",
    "        await ws.send(json.dumps({\"text\": \"\", \"flush\": False}))\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                msg = await ws.recv()\n",
    "                if isinstance(msg, bytes):\n",
    "                    msg = msg.decode(\"utf-8\")\n",
    "                payload = json.loads(msg)\n",
    "                audio_b64 = payload.get(\"audio\", \"\")\n",
    "                if not audio_b64:\n",
    "                    continue\n",
    "                audio = pcm_from_b64(audio_b64)\n",
    "                pcm_chunks.append(audio)\n",
    "                samples = audio.shape[0]\n",
    "                chunk_start_ms = (samples_so_far / float(sr)) * 1000.0\n",
    "                samples_so_far += samples\n",
    "\n",
    "                aln = payload.get(\"alignment\", {}) or {}\n",
    "                chars = aln.get(\"chars\", [])\n",
    "                starts = aln.get(\"char_start_times_ms\", [])\n",
    "                durs = aln.get(\"char_durations_ms\", [])\n",
    "                for ch, s, d in zip(chars, starts, durs):\n",
    "                    align_chars.append(ch)\n",
    "                    align_starts.append(chunk_start_ms + float(s))\n",
    "                    align_durs.append(float(d))\n",
    "\n",
    "                if t_first_audio is None:\n",
    "                    t_first_audio = time.perf_counter()\n",
    "                t_last_audio = time.perf_counter()\n",
    "        except websockets.ConnectionClosed:\n",
    "            pass\n",
    "\n",
    "    audio_out = np.concatenate(pcm_chunks) if pcm_chunks else np.zeros(0, dtype=np.float32)\n",
    "    return {\n",
    "        \"audio\": audio_out,\n",
    "        \"sr\": sr,\n",
    "        \"alignment\": {\n",
    "            \"chars\": align_chars,\n",
    "            \"char_start_times_ms\": align_starts,\n",
    "            \"char_durations_ms\": align_durs,\n",
    "        },\n",
    "        \"timing\": {\n",
    "            \"t_first_send\": t_first_send,\n",
    "            \"t_first_audio\": t_first_audio,\n",
    "            \"t_last_audio\": t_last_audio,\n",
    "            \"t_start\": t_start,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_quality(audio: np.ndarray) -> Dict[str, float]:\n",
    "    if audio.size == 0:\n",
    "        return {\"rms\": 0.0, \"peak\": 0.0, \"clipping_ratio\": 0.0, \"silence_ratio\": 1.0}\n",
    "    rms = float(np.sqrt(np.mean(audio ** 2)))\n",
    "    peak = float(np.max(np.abs(audio)))\n",
    "    clipping = float(np.mean(np.abs(audio) >= 0.999))\n",
    "    silence = float(np.mean(np.abs(audio) < 1e-4))\n",
    "    return {\n",
    "        \"rms\": rms,\n",
    "        \"peak\": peak,\n",
    "        \"clipping_ratio\": clipping,\n",
    "        \"silence_ratio\": silence,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ef300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, difflib\n",
    "\n",
    "def word_spans(text: str):\n",
    "    return [(m.start(), m.end()) for m in re.finditer(r\"\\S+\", text)]\n",
    "\n",
    "def char_to_word_map(spans, length):\n",
    "    mapping = [None] * length\n",
    "    for idx, (s, e) in enumerate(spans):\n",
    "        for i in range(s, e):\n",
    "            mapping[i] = idx\n",
    "    return mapping\n",
    "\n",
    "def align_words(aln: Dict[str, List], text: str):\n",
    "    chars = aln.get(\"chars\", [])\n",
    "    starts = aln.get(\"char_start_times_ms\", [])\n",
    "    durs = aln.get(\"char_durations_ms\", [])\n",
    "    spans = word_spans(text)\n",
    "    text_chars = list(text)\n",
    "    c2w = char_to_word_map(spans, len(text_chars))\n",
    "    matcher = difflib.SequenceMatcher(None, chars, text_chars)\n",
    "    word_times = {}\n",
    "    for block in matcher.get_matching_blocks():\n",
    "        a0, b0, size = block\n",
    "        for j in range(size):\n",
    "            si = a0 + j\n",
    "            ti = b0 + j\n",
    "            widx = c2w[ti] if 0 <= ti < len(c2w) else None\n",
    "            if widx is None:\n",
    "                continue\n",
    "            start = float(starts[si])\n",
    "            end = start + float(durs[si])\n",
    "            if widx not in word_times:\n",
    "                word_times[widx] = (start, end)\n",
    "            else:\n",
    "                s0, e0 = word_times[widx]\n",
    "                word_times[widx] = (min(s0, start), max(e0, end))\n",
    "    return word_times\n",
    "\n",
    "\n",
    "def compare_alignment_words(server_aln, ref_aln, text):\n",
    "    spans = word_spans(text)\n",
    "    sw = align_words(server_aln, text)\n",
    "    rw = align_words(ref_aln, text)\n",
    "    errors = []\n",
    "    matched = 0\n",
    "    for idx in range(len(spans)):\n",
    "        if idx in sw and idx in rw:\n",
    "            s_start, _ = sw[idx]\n",
    "            r_start, _ = rw[idx]\n",
    "            errors.append(s_start - r_start)\n",
    "            matched += 1\n",
    "    stats = {\n",
    "        \"words_total\": len(spans),\n",
    "        \"matched\": matched,\n",
    "        \"unmatched\": len(spans) - matched,\n",
    "    }\n",
    "    if errors:\n",
    "        errors = np.array(errors, dtype=np.float32)\n",
    "        pct = lambda p: float(np.percentile(errors, p))\n",
    "        stats.update(\n",
    "            {\n",
    "                \"mean_error_ms\": float(errors.mean()),\n",
    "                \"median_error_ms\": pct(50),\n",
    "                \"p90_error_ms\": pct(90),\n",
    "                \"p99_error_ms\": pct(99),\n",
    "                \"max_abs_error_ms\": float(np.max(np.abs(errors))),\n",
    "            }\n",
    "        )\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on sample_text.txt (server must be running)\n",
    "text_path = \"sample_text.txt\"\n",
    "with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "result = asyncio.run(run_session(sample_text, chunk_size=512, delay=0.0))\n",
    "audio = result[\"audio\"]\n",
    "sr = result[\"sr\"]\n",
    "alignment = result[\"alignment\"]\n",
    "timing = result[\"timing\"]\n",
    "\n",
    "# Latency metrics (client-side optimization metric)\n",
    "ttft = None\n",
    "if timing[\"t_first_audio\"] and timing[\"t_first_send\"]:\n",
    "    ttft = timing[\"t_first_audio\"] - timing[\"t_first_send\"]\n",
    "total = None\n",
    "if timing[\"t_last_audio\"] and timing[\"t_first_send\"]:\n",
    "    total = timing[\"t_last_audio\"] - timing[\"t_first_send\"]\n",
    "\n",
    "print({\n",
    "    \"ttft_s\": ttft,\n",
    "    \"total_s\": total,\n",
    "    \"tokens\": len(sample_text),\n",
    "    \"tokens_per_s\": (len(sample_text) / total) if total else None,\n",
    "})\n",
    "\n",
    "print(\"Audio quality:\", audio_quality(audio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f566aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAVE_WHISPERX:\n",
    "    print(\"Running WhisperX alignment for reference...\")\n",
    "    ax = WhisperXAligner()\n",
    "    ref_aln = ax.align(audio, sr, sample_text) or {}\n",
    "    stats = compare_alignment_words(alignment, ref_aln, sample_text)\n",
    "    print(\"Alignment stats (word-level vs WhisperX):\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"WhisperX not installed; skipping alignment comparison.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
